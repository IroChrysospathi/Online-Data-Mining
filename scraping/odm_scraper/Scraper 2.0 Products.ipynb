{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1cabc4",
   "metadata": {},
   "source": [
    "# Online Data Mining (ODM) \n",
    "## Competitive Benchmarking Tool (Bax-shop.nl vs Thomann, bol.com, MaxiAxi)\n",
    "\n",
    "### Academic & Business Objective\n",
    "This notebook implements a reproducible data-mining tool that:\n",
    "1) Collects **product offering and pricing signals** (incl. discounts, availability, delivery promise) for selected categories,\n",
    "2) Collects **retailer-level service propositions** (shipping/returns policies) and **expert support indicators**,\n",
    "3) Stores outputs into **CSV** and a **relational database** (SQLite by default, easily extendable to Postgres).\n",
    "\n",
    "The design aligns with the ODM module goal: **structured data acquisition** with relatively lightweight cleaning during/after scraping, enabling later statistical analysis.  \n",
    "Sources: Module guide and assignment description. (See provided docs.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7346aaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup OK. Timestamp: 2026-01-19T14:11:05.785260\n"
     ]
    }
   ],
   "source": [
    "# 0. Environment Setup\n",
    "\n",
    "\n",
    "# Core\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import hashlib\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime, timezone\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Database\n",
    "import sqlite3\n",
    "\n",
    "# Scrapy\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "print(\"Setup OK. Timestamp:\", datetime.now().isoformat())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de17fc2",
   "metadata": {},
   "source": [
    "# 1. Ethical and Operational Controls (Rate limiting, Scope, Logging)\n",
    "\n",
    "Web scraping must respect:\n",
    "- website terms,\n",
    "- `robots.txt`,\n",
    "- rate limiting,\n",
    "- and robust error handling.\n",
    "\n",
    "This notebook implements:\n",
    "- conservative concurrency,\n",
    "- download delay,\n",
    "- user-agent control,\n",
    "- and structured logging fields in the dataset.\n",
    "\n",
    "(ODM Lecture 1 & 2 content + module expectations.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0344af2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded. Retailers: ['maxiaxi'] Run: 20260119_141105\n",
      "Seed: https://www.maxiaxi.com/microfoons/?_gl=1*wun1p3*_up*MQ..*_gs*MQ..&gclid=CjwKCAiAybfLBhAjEiwAI0mBBtBabnYALUttCuiFDxKEWjAqyPC4M-DxsOfcTrDui7s_I3pyu6CXMxoClCwQAvD_BwE&gbraid=0AAAAADo6YHPbIVtlWk2zNZUOX0tH2Wu3R\n"
     ]
    }
   ],
   "source": [
    "# 1. Global Configuration (MaxiAxi - Microfoons only)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"currency\": \"EUR\",\n",
    "    \"max_pages_per_category\": 20,       # adjust if needed\n",
    "    \"download_delay_s\": 1.25,\n",
    "    \"concurrent_requests\": 4,\n",
    "    \"user_agent\": \"AUAS-ODM-Scraper/1.0 (educational use)\",\n",
    "    \"timeout_s\": 25,\n",
    "}\n",
    "\n",
    "# Task seed (use the exact URL you provided)\n",
    "MAXIAXI_MICROFOONS_URL = (\n",
    "    \"https://www.maxiaxi.com/microfoons/\"\n",
    "    \"?_gl=1*wun1p3*_up*MQ..*_gs*MQ..\"\n",
    "    \"&gclid=CjwKCAiAybfLBhAjEiwAI0mBBtBabnYALUttCuiFDxKEWjAqyPC4M-DxsOfcTrDui7s_I3pyu6CXMxoClCwQAvD_BwE\"\n",
    "    \"&gbraid=0AAAAADo6YHPbIVtlWk2zNZUOX0tH2Wu3R\"\n",
    ")\n",
    "\n",
    "# Only MaxiAxi\n",
    "RETAILERS = {\n",
    "    \"maxiaxi\": {\n",
    "        \"name\": \"MaxiAxi\",\n",
    "        \"base_url\": \"https://www.maxiaxi.com/\",\n",
    "        \"is_marketplace\": False,\n",
    "        \"policy_urls\": {\n",
    "            \"shipping_returns\": \"https://www.maxiaxi.com/klantenservice/\",\n",
    "        },\n",
    "        \"expert_support_urls\": {\n",
    "            \"advice\": \"https://www.maxiaxi.com/advies/\",\n",
    "        },\n",
    "        \"category_seeds\": {\n",
    "            \"microphones\": MAXIAXI_MICROFOONS_URL,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Only the selected task category\n",
    "CATEGORIES = [\n",
    "    {\"category_id\": 1, \"category_name\": \"Microphones\", \"key\": \"microphones\"},\n",
    "]\n",
    "\n",
    "print(\"Config loaded. Retailers:\", list(RETAILERS.keys()), \"Run:\", RUN_ID)\n",
    "print(\"Seed:\", MAXIAXI_MICROFOONS_URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a8e12",
   "metadata": {},
   "source": [
    "# 2. Data Model (ERD-aligned) — Minimal Working Subset\n",
    "\n",
    "The full ERD in your report can be extensive; for the scraper tool we implement a *minimal, functional subset* that already supports:\n",
    "- Retailer-level pages (policy/support)\n",
    "- Product listing → product detail\n",
    "- Offer observations (price, reference price, discount %, stock, delivery promise)\n",
    "- Time-stamped snapshots (observed_at)\n",
    "\n",
    "This notebook stores:\n",
    "- `retailer`\n",
    "- `category`\n",
    "- `product_page` (retailer-specific product identity)\n",
    "- `offer_observation` (time series)\n",
    "\n",
    "This structure is aligned to the ERD principle: separate **product pages** from **time-stamped observations** and support later relational expansion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2e045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# 1. Run identifier\n",
    "\n",
    "try:\n",
    "    RUN_ID\n",
    "except NameError:\n",
    "    RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# 2. Desktop path\n",
    "\n",
    "DESKTOP_DIR = Path(\"/Users/feddekoster/Desktop\")\n",
    "\n",
    "if not DESKTOP_DIR.exists():\n",
    "    raise OSError(\n",
    "        f\"Desktop directory not found at {DESKTOP_DIR}. \"\n",
    "        \"Check that the username is correct or that Desktop exists.\"\n",
    "    )\n",
    "\n",
    "# Test write permissions explicitly\n",
    "try:\n",
    "    test_file = DESKTOP_DIR / \".odm_write_test\"\n",
    "    test_file.write_text(\"ok\", encoding=\"utf-8\")\n",
    "    test_file.unlink()\n",
    "except Exception as e:\n",
    "    raise PermissionError(\n",
    "        \"Desktop is not writable by this Python/Jupyter process.\\n\\n\"\n",
    "        \"macOS fix:\\n\"\n",
    "        \"System Settings → Privacy & Security → Files and Folders (or Full Disk Access)\\n\"\n",
    "        \"→ Enable Desktop access for the app running Jupyter (Terminal / VS Code / Anaconda).\\n\\n\"\n",
    "        f\"Original error: {e}\"\n",
    "    )\n",
    "\n",
    "# 3. Assignment output folder\n",
    "\n",
    "OUT_DIR = DESKTOP_DIR / \"ODM_Assignment2\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DB_PATH = OUT_DIR / f\"odm_competitor_benchmark_{RUN_ID}.sqlite\"\n",
    "\n",
    "\n",
    "# 4. Database schema\n",
    "\n",
    "DDL = \"\"\"\n",
    "PRAGMA foreign_keys = ON;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS retailer (\n",
    "  retailer_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "  retailer_key TEXT UNIQUE NOT NULL,\n",
    "  name TEXT NOT NULL,\n",
    "  base_url TEXT NOT NULL,\n",
    "  is_marketplace INTEGER NOT NULL,\n",
    "  created_at TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS category (\n",
    "  category_id INTEGER PRIMARY KEY,\n",
    "  category_key TEXT UNIQUE NOT NULL,\n",
    "  category_name TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS product_page (\n",
    "  product_page_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "  retailer_id INTEGER NOT NULL,\n",
    "  category_id INTEGER NOT NULL,\n",
    "  url TEXT NOT NULL,\n",
    "  retailer_product_id TEXT,\n",
    "  page_title TEXT,\n",
    "  brand TEXT,\n",
    "  gtin_ean TEXT,\n",
    "  last_seen_at TEXT NOT NULL,\n",
    "  UNIQUE(retailer_id, url),\n",
    "  FOREIGN KEY(retailer_id) REFERENCES retailer(retailer_id),\n",
    "  FOREIGN KEY(category_id) REFERENCES category(category_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS offer_observation (\n",
    "  observation_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "  product_page_id INTEGER NOT NULL,\n",
    "  observed_at TEXT NOT NULL,\n",
    "  price_current REAL,\n",
    "  price_reference REAL,\n",
    "  discount_pct REAL,\n",
    "  promo_flag INTEGER,\n",
    "  stock_text_raw TEXT,\n",
    "  delivery_promise_text TEXT,\n",
    "  currency TEXT NOT NULL,\n",
    "  http_status INTEGER,\n",
    "  scrape_run_id TEXT NOT NULL,\n",
    "  FOREIGN KEY(product_page_id) REFERENCES product_page(product_page_id)\n",
    ");\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS retailer_page_capture (\n",
    "  capture_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "  retailer_id INTEGER NOT NULL,\n",
    "  page_type TEXT NOT NULL,\n",
    "  source_url TEXT NOT NULL,\n",
    "  captured_at TEXT NOT NULL,\n",
    "  content_text TEXT,\n",
    "  http_status INTEGER,\n",
    "  scrape_run_id TEXT NOT NULL,\n",
    "  FOREIGN KEY(retailer_id) REFERENCES retailer(retailer_id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# 5. Create database\n",
    "\n",
    "def db_connect(path: Path):\n",
    "    con = sqlite3.connect(str(path))\n",
    "    con.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "    return con\n",
    "\n",
    "with db_connect(DB_PATH) as con:\n",
    "    con.executescript(DDL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcda0cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_connect(path: Path = DB_PATH):\n",
    "    con = sqlite3.connect(str(path))\n",
    "    con.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "    return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e53f3894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# 2b. Seed reference tables\n",
    "\n",
    "def upsert_retailers(con):\n",
    "    now = datetime.now(timezone.utc).isoformat()\n",
    "    for r_key, r in RETAILERS.items():\n",
    "        con.execute(\"\"\"\n",
    "            INSERT INTO retailer (retailer_key, name, base_url, is_marketplace, created_at)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "            ON CONFLICT(retailer_key) DO UPDATE SET\n",
    "                name=excluded.name,\n",
    "                base_url=excluded.base_url,\n",
    "                is_marketplace=excluded.is_marketplace\n",
    "        \"\"\", (r_key, r[\"name\"], r[\"base_url\"], int(r[\"is_marketplace\"]), now))\n",
    "    con.commit()\n",
    "\n",
    "def seed_categories(con):\n",
    "    for c in CATEGORIES:\n",
    "        con.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO category (category_id, category_key, category_name)\n",
    "            VALUES (?, ?, ?)\n",
    "        \"\"\", (c[\"category_id\"], c[\"key\"], c[\"category_name\"]))\n",
    "    con.commit()\n",
    "\n",
    "with db_connect() as con:  \n",
    "    upsert_retailers(con)\n",
    "    seed_categories(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b592d563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "OUTPUT_DIR = OUT_DIR  \n",
    "PROD_CSV_PATH = OUTPUT_DIR / f\"product_observations_{RUN_ID}.csv\"\n",
    "RET_CSV_PATH  = OUTPUT_DIR / f\"retailer_pages_{RUN_ID}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59766277",
   "metadata": {},
   "source": [
    "# 3. Helper Functions (Normalization, Discount Formula, Safe Text Extraction)\n",
    "\n",
    "## Discount depth formula\n",
    "If a reference/list price is available:\n",
    "\n",
    "\\[\n",
    "\\text{Discount \\%} = \\frac{\\text{Reference price} - \\text{Current price}}{\\text{Reference price}} \\times 100\n",
    "\\]\n",
    "\n",
    "If reference price is missing or equals 0, discount is undefined and set to null.\n",
    "\n",
    "We also normalize:\n",
    "- currency symbols,\n",
    "- thousand/decimal separators,\n",
    "- whitespace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd99f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Helper functions (Scraper 2.0 - MaxiAxi Microfoons)\n",
    "\n",
    "import re\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
    "\n",
    "def clean_text(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    x = str(x)\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x if x else None\n",
    "\n",
    "def parse_price(price_text):\n",
    "    \"\"\"\n",
    "    Parse price strings like:\n",
    "    '€ 39,95' -> 39.95\n",
    "    '39,95'   -> 39.95\n",
    "    \"\"\"\n",
    "    if not price_text:\n",
    "        return None\n",
    "    t = clean_text(price_text)\n",
    "    if not t:\n",
    "        return None\n",
    "\n",
    "    # Keep digits, comma, dot\n",
    "    t = re.sub(r\"[^\\d,\\.]\", \"\", t)\n",
    "\n",
    "    # If comma is used as decimal separator\n",
    "    # Example: 39,95 -> 39.95\n",
    "    if t.count(\",\") == 1 and t.count(\".\") == 0:\n",
    "        t = t.replace(\",\", \".\")\n",
    "    # If both appear, assume dot is thousands and comma is decimal: 1.299,95 -> 1299.95\n",
    "    elif t.count(\",\") == 1 and t.count(\".\") >= 1:\n",
    "        t = t.replace(\".\", \"\").replace(\",\", \".\")\n",
    "\n",
    "    try:\n",
    "        return float(t)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calc_discount_pct(price_reference, price_current):\n",
    "    if price_reference is None or price_current is None:\n",
    "        return None\n",
    "    if price_reference <= 0:\n",
    "        return None\n",
    "    if price_current >= price_reference:\n",
    "        return 0.0\n",
    "    return round((price_reference - price_current) / price_reference * 100.0, 2)\n",
    "\n",
    "def strip_tracking(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Benchmark-inspired URL normaliser (based on your classmate’s bol_products.py).\n",
    "    Removes common marketing/tracking parameters so URLs are stable across runs.\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return url\n",
    "    try:\n",
    "        p = urlparse(url)\n",
    "        q = parse_qs(p.query)\n",
    "\n",
    "        drop_keys = {\n",
    "            \"gclid\", \"gbraid\", \"wbraid\", \"fbclid\",\n",
    "            \"utm_source\", \"utm_medium\", \"utm_campaign\", \"utm_term\", \"utm_content\",\n",
    "            \"_gl\", \"_ga\", \"_gid\", \"mc_cid\", \"mc_eid\",\n",
    "            \"ref\", \"cid\", \"source\"\n",
    "        }\n",
    "\n",
    "        for k in list(q.keys()):\n",
    "            if k.lower() in drop_keys:\n",
    "                q.pop(k, None)\n",
    "\n",
    "        new_query = urlencode(q, doseq=True)\n",
    "        return urlunparse((p.scheme, p.netloc, p.path, p.params, new_query, p.fragment))\n",
    "    except Exception:\n",
    "        return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ce38e",
   "metadata": {},
   "source": [
    "# 4. Scrapy Items (Structured Output)\n",
    "\n",
    "We emit two item types:\n",
    "1) `ProductObservationItem` — for product detail pages (offer observations).\n",
    "2) `RetailerPageItem` — for policy/support pages (retailer-level capture).\n",
    "\n",
    "This directly supports the assignment requirement: deliver both **code (system)** and **data (datasets)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5eb45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scrapy Items\n",
    "\n",
    "class ProductObservationItem(scrapy.Item):\n",
    "    retailer_key = scrapy.Field()\n",
    "    category_key = scrapy.Field()\n",
    "    product_url = scrapy.Field()\n",
    "    page_title = scrapy.Field()\n",
    "    retailer_product_id = scrapy.Field()\n",
    "    brand = scrapy.Field()\n",
    "    gtin_ean = scrapy.Field()\n",
    "\n",
    "    price_current = scrapy.Field()\n",
    "    price_reference = scrapy.Field()\n",
    "    discount_pct = scrapy.Field()\n",
    "    promo_flag = scrapy.Field()\n",
    "\n",
    "    stock_text_raw = scrapy.Field()\n",
    "    delivery_promise_text = scrapy.Field()\n",
    "\n",
    "    observed_at = scrapy.Field()\n",
    "    http_status = scrapy.Field()\n",
    "    scrape_run_id = scrapy.Field()\n",
    "\n",
    "class RetailerPageItem(scrapy.Item):\n",
    "    retailer_key = scrapy.Field()\n",
    "    page_type = scrapy.Field()     # 'policy' or 'expert_support'\n",
    "    source_url = scrapy.Field()\n",
    "    captured_at = scrapy.Field()\n",
    "    content_text = scrapy.Field()\n",
    "    http_status = scrapy.Field()\n",
    "    scrape_run_id = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323bd32",
   "metadata": {},
   "source": [
    "# 5. Spider Implementation (Generic Baseline)\n",
    "\n",
    "This spider is **generic**: it:\n",
    "- starts from category listing pages for each retailer,\n",
    "- collects product detail URLs from listing pages,\n",
    "- visits product pages to extract benchmark variables.\n",
    "\n",
    "Important:\n",
    "- Each retailer’s HTML differs; you must adapt selectors per domain.\n",
    "- The notebook provides **clear selector placeholders** and a repeatable pattern.\n",
    "- Start with 1 retailer + 1 category, validate output, then expand.\n",
    "\n",
    "For dynamic pages:\n",
    "- Convert `scrapy.Request` to `SeleniumRequest` and add waits/scripts when required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f39e80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Spider (Scraper 2.0 - MaxiAxi Microfoons)\n",
    "\n",
    "import scrapy\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "\n",
    "class CompetitorBenchmarkSpider(scrapy.Spider):\n",
    "    name = \"maxiaxi_microfoons\"\n",
    "    allowed_domains = [\"www.maxiaxi.com\", \"maxiaxi.com\"]\n",
    "\n",
    "    custom_settings = {\n",
    "        \"USER_AGENT\": CONFIG[\"user_agent\"],\n",
    "        \"DOWNLOAD_DELAY\": CONFIG[\"download_delay_s\"],\n",
    "        \"CONCURRENT_REQUESTS\": CONFIG[\"concurrent_requests\"],\n",
    "        \"LOG_LEVEL\": \"INFO\",\n",
    "        \"DOWNLOAD_TIMEOUT\": CONFIG[\"timeout_s\"],\n",
    "        \"ROBOTSTXT_OBEY\": True,\n",
    "        \"ITEM_PIPELINES\": {\n",
    "            \"__main__.SQLitePipeline\": 300,\n",
    "            \"__main__.CSVPipeline\": 400,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def start_requests(self):\n",
    "        r_key = \"maxiaxi\"\n",
    "        r = RETAILERS[r_key]\n",
    "\n",
    "        # Retailer-level pages (optional but useful for ERD completeness)\n",
    "        for page_type, url in (r.get(\"policy_urls\") or {}).items():\n",
    "            yield scrapy.Request(\n",
    "                url=strip_tracking(url),\n",
    "                callback=self.parse_retailer_page,\n",
    "                meta={\"retailer_key\": r_key, \"page_type\": page_type}\n",
    "            )\n",
    "\n",
    "        for page_type, url in (r.get(\"expert_support_urls\") or {}).items():\n",
    "            yield scrapy.Request(\n",
    "                url=strip_tracking(url),\n",
    "                callback=self.parse_retailer_page,\n",
    "                meta={\"retailer_key\": r_key, \"page_type\": page_type}\n",
    "            )\n",
    "\n",
    "        # Category seed: microphones only\n",
    "        seed = r[\"category_seeds\"][\"microphones\"]\n",
    "        yield scrapy.Request(\n",
    "            url=strip_tracking(seed),\n",
    "            callback=self.parse_listing,\n",
    "            meta={\"retailer_key\": r_key, \"category_key\": \"microphones\", \"page_no\": 1}\n",
    "        )\n",
    "\n",
    "    def parse_retailer_page(self, response):\n",
    "        retailer_key = response.meta[\"retailer_key\"]\n",
    "        page_type = response.meta[\"page_type\"]\n",
    "\n",
    "        body_text = clean_text(\" \".join(response.css(\"body *::text\").getall()))\n",
    "        if body_text:\n",
    "            body_text = body_text[:5000]\n",
    "\n",
    "        yield RetailerPageItem(\n",
    "            retailer_key=retailer_key,\n",
    "            page_type=page_type,\n",
    "            source_url=strip_tracking(response.url),\n",
    "            captured_at=datetime.now(timezone.utc).isoformat(),\n",
    "            content_text=body_text,\n",
    "            http_status=response.status,\n",
    "            scrape_run_id=RUN_ID\n",
    "        )\n",
    "\n",
    "    def parse_listing(self, response):\n",
    "        retailer_key = response.meta[\"retailer_key\"]\n",
    "        category_key = response.meta[\"category_key\"]\n",
    "        page_no = response.meta.get(\"page_no\", 1)\n",
    "\n",
    "        self.logger.info(\"LISTING page=%s status=%s url=%s\", page_no, response.status, response.url)\n",
    "\n",
    "        raw_links = response.css(\n",
    "            \"ol.products li.product-item a.product-item-link::attr(href),\"\n",
    "            \"a.product-item-link::attr(href)\"\n",
    "        ).getall()\n",
    "\n",
    "        links = [strip_tracking(urljoin(response.url, h)) for h in raw_links if h]\n",
    "        links = list(dict.fromkeys(links))\n",
    "\n",
    "        def is_product_url(u: str) -> bool:\n",
    "            if not u or \"maxiaxi.com\" not in u:\n",
    "                return False\n",
    "\n",
    "            low = u.lower()\n",
    "\n",
    "            # Drop category itself and irrelevant areas\n",
    "            if \"/microfoons/\" in low:\n",
    "                return False\n",
    "            if any(x in low for x in [\"/klantenservice\", \"/advies\", \"/blog\", \"/account\", \"/checkout\"]):\n",
    "                return False\n",
    "\n",
    "            # MaxiAxi product URLs often look like:\n",
    "            # https://www.maxiaxi.com/<slug>/\n",
    "            return bool(re.match(r\"^https://www\\.maxiaxi\\.com/[^/]+/?$\", u))\n",
    "\n",
    "        product_links = [u for u in links if is_product_url(u)]\n",
    "\n",
    "        self.logger.info(\"LISTING found_links=%s product_links=%s\", len(links), len(product_links))\n",
    "\n",
    "        for u in product_links:\n",
    "            yield scrapy.Request(\n",
    "                url=u,\n",
    "                callback=self.parse_product,\n",
    "                meta={\"retailer_key\": retailer_key, \"category_key\": category_key}\n",
    "            )\n",
    "\n",
    "        # Pagination\n",
    "        if page_no < CONFIG[\"max_pages_per_category\"]:\n",
    "            next_href = response.css(\n",
    "                \"li.pages-item-next a::attr(href), a.action.next::attr(href)\"\n",
    "            ).get()\n",
    "\n",
    "            if next_href:\n",
    "                next_url = strip_tracking(urljoin(response.url, next_href))\n",
    "                yield scrapy.Request(\n",
    "                    url=next_url,\n",
    "                    callback=self.parse_listing,\n",
    "                    meta={\"retailer_key\": retailer_key, \"category_key\": category_key, \"page_no\": page_no + 1}\n",
    "                )\n",
    "\n",
    "    def parse_product(self, response):\n",
    "        retailer_key = response.meta[\"retailer_key\"]\n",
    "        category_key = response.meta[\"category_key\"]\n",
    "\n",
    "        product_url = strip_tracking(response.url)\n",
    "\n",
    "        page_title = clean_text(response.css(\"h1.page-title span::text, h1::text\").get())\n",
    "        if not page_title:\n",
    "            page_title = clean_text(response.css(\"title::text\").get())\n",
    "\n",
    "        # Prices\n",
    "        price_cur_raw = clean_text(response.css(\n",
    "            \"span.price-final_price span.price::text,\"\n",
    "            \"span.special-price span.price::text,\"\n",
    "            \"span.price-wrapper span.price::text\"\n",
    "        ).get() or \"\")\n",
    "\n",
    "        price_ref_raw = clean_text(response.css(\n",
    "            \"span.old-price span.price::text,\"\n",
    "            \"span.regular-price span.price::text\"\n",
    "        ).get() or \"\")\n",
    "\n",
    "        price_current = parse_price(price_cur_raw)\n",
    "        price_reference = parse_price(price_ref_raw)\n",
    "        discount_pct = calc_discount_pct(price_reference, price_current)\n",
    "        promo_flag = int(discount_pct is not None and discount_pct > 0)\n",
    "\n",
    "        # Stock\n",
    "        stock_text_raw = clean_text(\" \".join(response.css(\n",
    "            \".stock.available *::text, .stock.unavailable *::text, .availability *::text\"\n",
    "        ).getall()))\n",
    "\n",
    "        # Delivery promise\n",
    "        delivery_promise_text = clean_text(\" \".join(response.xpath(\n",
    "            \"//*[contains(normalize-space(), 'Bestel voor') or contains(normalize-space(), 'morgen')]/text()\"\n",
    "        ).getall()))\n",
    "\n",
    "        # Specs extraction\n",
    "        def value_after_label(label: str):\n",
    "            v = response.xpath(\n",
    "                f\"//th[normalize-space()='{label}']/following-sibling::td[1]//text()\"\n",
    "            ).getall()\n",
    "            if v:\n",
    "                return clean_text(\" \".join(v))\n",
    "\n",
    "            v2 = response.xpath(\n",
    "                f\"//*[normalize-space()='{label}']/following::*[1]//text()\"\n",
    "            ).getall()\n",
    "            return clean_text(\" \".join(v2)) if v2 else None\n",
    "\n",
    "        brand = value_after_label(\"Merk\")\n",
    "        retailer_product_id = value_after_label(\"SKU\")     # mapping SKU -> retailer_product_id\n",
    "        gtin_ean = value_after_label(\"EAN Code\")\n",
    "\n",
    "        yield ProductObservationItem(\n",
    "            retailer_key=retailer_key,\n",
    "            category_key=category_key,\n",
    "            product_url=product_url,\n",
    "            page_title=page_title,\n",
    "            retailer_product_id=retailer_product_id,\n",
    "            brand=brand,\n",
    "            gtin_ean=gtin_ean,\n",
    "\n",
    "            price_current=price_current,\n",
    "            price_reference=price_reference,\n",
    "            discount_pct=discount_pct,\n",
    "            promo_flag=promo_flag,\n",
    "\n",
    "            stock_text_raw=stock_text_raw,\n",
    "            delivery_promise_text=delivery_promise_text,\n",
    "\n",
    "            observed_at=datetime.now(timezone.utc).isoformat(),\n",
    "            http_status=response.status,\n",
    "            scrape_run_id=RUN_ID\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48ba59",
   "metadata": {},
   "source": [
    "# 6. Pipelines: Store into SQLite + CSV\n",
    "\n",
    "Two outputs are required for the assignment:\n",
    "- datasets (CSV),\n",
    "- structured storage (database preferred).\n",
    "\n",
    "We implement both:\n",
    "- SQLite insertion (relational),\n",
    "- CSV exports (data deliverable + transparency).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09e73f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 6. Pipelines\n",
    "\n",
    "\n",
    "class SQLitePipeline:\n",
    "    def open_spider(self, spider):\n",
    "        self.con = db_connect()\n",
    "        self.cur = self.con.cursor()\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.con.commit()\n",
    "        self.con.close()\n",
    "\n",
    "    def _get_retailer_id(self, retailer_key):\n",
    "        row = self.cur.execute(\n",
    "            \"SELECT retailer_id FROM retailer WHERE retailer_key=?\",\n",
    "            (retailer_key,)\n",
    "        ).fetchone()\n",
    "        if not row:\n",
    "            raise RuntimeError(\n",
    "                f\"Retailer '{retailer_key}' not found in DB. \"\n",
    "                \"Did you run the seeding cell (upsert_retailers)?\"\n",
    "            )\n",
    "        return row[0]\n",
    "\n",
    "    def _get_category_id(self, category_key):\n",
    "        row = self.cur.execute(\n",
    "            \"SELECT category_id FROM category WHERE category_key=?\",\n",
    "            (category_key,)\n",
    "        ).fetchone()\n",
    "        if not row:\n",
    "            raise RuntimeError(\n",
    "                f\"Category '{category_key}' not found in DB. \"\n",
    "                \"Did you run the seeding cell (seed_categories)?\"\n",
    "            )\n",
    "        return row[0]\n",
    "\n",
    "    def _upsert_product_page(self, retailer_id, category_id, item):\n",
    "        now = datetime.now(timezone.utc).isoformat()\n",
    "        self.cur.execute(\"\"\"\n",
    "            INSERT INTO product_page (\n",
    "              retailer_id, category_id, url, retailer_product_id, page_title, brand, gtin_ean, last_seen_at\n",
    "            )\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ON CONFLICT(retailer_id, url) DO UPDATE SET\n",
    "                retailer_product_id=excluded.retailer_product_id,\n",
    "                page_title=excluded.page_title,\n",
    "                brand=excluded.brand,\n",
    "                gtin_ean=excluded.gtin_ean,\n",
    "                last_seen_at=excluded.last_seen_at\n",
    "        \"\"\", (\n",
    "            retailer_id, category_id,\n",
    "            item.get(\"product_url\"),\n",
    "            item.get(\"retailer_product_id\"),\n",
    "            item.get(\"page_title\"),\n",
    "            item.get(\"brand\"),\n",
    "            item.get(\"gtin_ean\"),\n",
    "            now\n",
    "        ))\n",
    "        row = self.cur.execute(\n",
    "            \"SELECT product_page_id FROM product_page WHERE retailer_id=? AND url=?\",\n",
    "            (retailer_id, item.get(\"product_url\"))\n",
    "        ).fetchone()\n",
    "        return row[0]\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        d = dict(item)\n",
    "\n",
    "        # Retailer-level pages (policy/support)\n",
    "        if isinstance(item, RetailerPageItem):\n",
    "            retailer_id = self._get_retailer_id(d[\"retailer_key\"])\n",
    "            self.cur.execute(\"\"\"\n",
    "                INSERT INTO retailer_page_capture (\n",
    "                  retailer_id, page_type, source_url, captured_at, content_text, http_status, scrape_run_id\n",
    "                )\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                retailer_id,\n",
    "                d[\"page_type\"],\n",
    "                d[\"source_url\"],\n",
    "                d[\"captured_at\"],\n",
    "                d.get(\"content_text\"),\n",
    "                d.get(\"http_status\"),\n",
    "                d[\"scrape_run_id\"]\n",
    "            ))\n",
    "            self.con.commit()\n",
    "            return item\n",
    "\n",
    "        # Product observation (price/stock/delivery)\n",
    "        if isinstance(item, ProductObservationItem):\n",
    "            retailer_id = self._get_retailer_id(d[\"retailer_key\"])\n",
    "            category_id = self._get_category_id(d[\"category_key\"])\n",
    "            pp_id = self._upsert_product_page(retailer_id, category_id, d)\n",
    "\n",
    "            self.cur.execute(\"\"\"\n",
    "                INSERT INTO offer_observation (\n",
    "                  product_page_id, observed_at, price_current, price_reference, discount_pct,\n",
    "                  promo_flag, stock_text_raw, delivery_promise_text, currency, http_status, scrape_run_id\n",
    "                )\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\", (\n",
    "                pp_id,\n",
    "                d[\"observed_at\"],\n",
    "                d.get(\"price_current\"),\n",
    "                d.get(\"price_reference\"),\n",
    "                d.get(\"discount_pct\"),\n",
    "                d.get(\"promo_flag\"),\n",
    "                d.get(\"stock_text_raw\"),\n",
    "                d.get(\"delivery_promise_text\"),\n",
    "                CONFIG[\"currency\"],\n",
    "                d.get(\"http_status\"),\n",
    "                d[\"scrape_run_id\"]\n",
    "            ))\n",
    "            self.con.commit()\n",
    "            return item\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "class CSVPipeline:\n",
    "    def open_spider(self, spider):\n",
    "        self.products = []\n",
    "        self.pages = []\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        if self.products:\n",
    "            dfp = pd.DataFrame(self.products)\n",
    "            dfp.to_csv(PROD_CSV_PATH, index=False)\n",
    "\n",
    "        if self.pages:\n",
    "            dfr = pd.DataFrame(self.pages)\n",
    "            dfr.to_csv(RET_CSV_PATH, index=False)\n",
    "\n",
    "        print(\"CSV saved to:\")\n",
    "        if self.products:\n",
    "            print(\"-\", PROD_CSV_PATH)\n",
    "        if self.pages:\n",
    "            print(\"-\", RET_CSV_PATH)\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if isinstance(item, ProductObservationItem):\n",
    "            self.products.append(dict(item))\n",
    "        elif isinstance(item, RetailerPageItem):\n",
    "            self.pages.append(dict(item))\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b4a74",
   "metadata": {},
   "source": [
    "# 7. Run the Scraper\n",
    "\n",
    "Important:\n",
    "- Start small (1 retailer + 1 category) to validate selectors.\n",
    "- Then expand categories/pages once extraction quality is correct.\n",
    "\n",
    "If you see many irrelevant URLs in listing extraction:\n",
    "- refine the listing selectors immediately (this is normal in early sprint).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8bcf95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-19 14:11:05 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2026-01-19 14:11:05 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.13.8, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 24.11.0, Python 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 11:23:37) [Clang 14.0.6 ], pyOpenSSL 25.0.0 (OpenSSL 3.0.17 1 Jul 2025), cryptography 44.0.1, Platform macOS-15.6.1-arm64-arm-64bit-Mach-O\n",
      "2026-01-19 14:11:05 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2026-01-19 14:11:05 [scrapy.extensions.telnet] INFO: Telnet Password: 33dded98808e5787\n",
      "2026-01-19 14:11:05 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2026-01-19 14:11:05 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'CONCURRENT_REQUESTS': 4,\n",
      " 'DOWNLOAD_DELAY': 1.25,\n",
      " 'DOWNLOAD_TIMEOUT': 25,\n",
      " 'LOG_LEVEL': 'INFO',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'USER_AGENT': 'AUAS-ODM-Scraper/1.0 (educational use)'}\n",
      "2026-01-19 14:11:06 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2026-01-19 14:11:06 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2026-01-19 14:11:06 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "['__main__.SQLitePipeline', '__main__.CSVPipeline']\n",
      "2026-01-19 14:11:06 [scrapy.core.engine] INFO: Spider opened\n",
      "2026-01-19 14:11:06 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2026-01-19 14:11:06 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6033\n",
      "2026-01-19 14:11:08 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://www.maxiaxi.com/advies/>: HTTP status code is not handled or not allowed\n",
      "2026-01-19 14:11:09 [maxiaxi_microfoons] INFO: LISTING page=1 status=200 url=https://www.maxiaxi.com/microfoons/\n",
      "2026-01-19 14:11:09 [maxiaxi_microfoons] INFO: LISTING found_links=24 product_links=24\n",
      "2026-01-19 14:11:13 [maxiaxi_microfoons] INFO: LISTING page=2 status=200 url=https://www.maxiaxi.com/microfoons/?p=2\n",
      "2026-01-19 14:11:13 [maxiaxi_microfoons] INFO: LISTING found_links=24 product_links=24\n",
      "2026-01-19 14:11:20 [maxiaxi_microfoons] INFO: LISTING page=3 status=200 url=https://www.maxiaxi.com/microfoons/?p=3\n",
      "2026-01-19 14:11:20 [maxiaxi_microfoons] INFO: LISTING found_links=24 product_links=24\n",
      "2026-01-19 14:11:31 [maxiaxi_microfoons] INFO: LISTING page=4 status=200 url=https://www.maxiaxi.com/microfoons/?p=4\n",
      "2026-01-19 14:11:31 [maxiaxi_microfoons] INFO: LISTING found_links=24 product_links=24\n",
      "2026-01-19 14:11:41 [maxiaxi_microfoons] INFO: LISTING page=5 status=200 url=https://www.maxiaxi.com/microfoons/?p=5\n",
      "2026-01-19 14:11:41 [maxiaxi_microfoons] INFO: LISTING found_links=24 product_links=24\n",
      "2026-01-19 14:11:51 [maxiaxi_microfoons] INFO: LISTING page=6 status=200 url=https://www.maxiaxi.com/microfoons/?p=6\n",
      "2026-01-19 14:11:51 [maxiaxi_microfoons] INFO: LISTING found_links=24 product_links=24\n",
      "2026-01-19 14:11:58 [maxiaxi_microfoons] INFO: LISTING page=7 status=200 url=https://www.maxiaxi.com/microfoons/?p=7\n",
      "2026-01-19 14:11:58 [maxiaxi_microfoons] INFO: LISTING found_links=24 product_links=24\n",
      "2026-01-19 14:12:06 [scrapy.extensions.logstats] INFO: Crawled 37 pages (at 37 pages/min), scraped 28 items (at 28 items/min)\n",
      "2026-01-19 14:12:06 [maxiaxi_microfoons] INFO: LISTING page=8 status=200 url=https://www.maxiaxi.com/microfoons/?p=8\n",
      "2026-01-19 14:12:06 [maxiaxi_microfoons] INFO: LISTING found_links=24 product_links=24\n",
      "2026-01-19 14:12:13 [maxiaxi_microfoons] INFO: LISTING page=9 status=200 url=https://www.maxiaxi.com/microfoons/?p=9\n",
      "2026-01-19 14:12:13 [maxiaxi_microfoons] INFO: LISTING found_links=20 product_links=20\n",
      "2026-01-19 14:13:06 [scrapy.extensions.logstats] INFO: Crawled 79 pages (at 42 pages/min), scraped 68 items (at 40 items/min)\n",
      "2026-01-19 14:14:06 [scrapy.extensions.logstats] INFO: Crawled 119 pages (at 40 pages/min), scraped 107 items (at 39 items/min)\n",
      "2026-01-19 14:15:06 [scrapy.extensions.logstats] INFO: Crawled 158 pages (at 39 pages/min), scraped 147 items (at 40 items/min)\n",
      "2026-01-19 14:16:06 [scrapy.extensions.logstats] INFO: Crawled 196 pages (at 38 pages/min), scraped 185 items (at 38 items/min)\n",
      "2026-01-19 14:16:09 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2026-01-19 14:16:09 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/exception_count': 25,\n",
      " 'downloader/exception_type_count/scrapy.exceptions.IgnoreRequest': 25,\n",
      " 'downloader/request_bytes': 92904,\n",
      " 'downloader/request_count': 199,\n",
      " 'downloader/request_method_count/GET': 199,\n",
      " 'downloader/response_bytes': 9603720,\n",
      " 'downloader/response_count': 199,\n",
      " 'downloader/response_status_count/200': 198,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 303.202207,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2026, 1, 19, 13, 16, 9, 246179, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 57848086,\n",
      " 'httpcompression/response_count': 198,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/404': 1,\n",
      " 'item_scraped_count': 188,\n",
      " 'items_per_minute': None,\n",
      " 'log_count/INFO': 34,\n",
      " 'memusage/max': 272924672,\n",
      " 'memusage/startup': 184172544,\n",
      " 'request_depth_max': 9,\n",
      " 'response_received_count': 199,\n",
      " 'responses_per_minute': None,\n",
      " 'robotstxt/forbidden': 25,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/200': 1,\n",
      " 'scheduler/dequeued': 223,\n",
      " 'scheduler/dequeued/memory': 223,\n",
      " 'scheduler/enqueued': 223,\n",
      " 'scheduler/enqueued/memory': 223,\n",
      " 'start_time': datetime.datetime(2026, 1, 19, 13, 11, 6, 43972, tzinfo=datetime.timezone.utc)}\n",
      "2026-01-19 14:16:09 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved to:\n",
      "- /Users/feddekoster/Desktop/ODM_Assignment2/product_observations_20260119_141105.csv\n",
      "- /Users/feddekoster/Desktop/ODM_Assignment2/retailer_pages_20260119_141105.csv\n",
      "Scrape finished. Outputs saved:\n",
      "- /Users/feddekoster/Desktop/ODM_Assignment2/product_observations_20260119_141105.csv\n",
      "- /Users/feddekoster/Desktop/ODM_Assignment2/retailer_pages_20260119_141105.csv\n",
      "- /Users/feddekoster/Desktop/ODM_Assignment2/odm_competitor_benchmark_20260119_141105.sqlite\n"
     ]
    }
   ],
   "source": [
    "# 7. Run Spider (ONE run method only)\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# IMPORTANT:\n",
    "# Twisted reactor can only run once per Jupyter kernel.\n",
    "# If you want to run the spider again:\n",
    "# Kernel -> Restart Kernel -> Run all cells once\n",
    "\n",
    "process = CrawlerProcess(settings={})\n",
    "process.crawl(CompetitorBenchmarkSpider)\n",
    "process.start()\n",
    "\n",
    "print(\"Scrape finished. Outputs saved:\")\n",
    "print(\"-\", PROD_CSV_PATH)\n",
    "print(\"-\", RET_CSV_PATH)\n",
    "print(\"-\", DB_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be2cf22",
   "metadata": {},
   "source": [
    "# 8. Post-run QA: Basic Data Quality Checks\n",
    "\n",
    "We validate:\n",
    "- counts per retailer,\n",
    "- missing price ratios,\n",
    "- basic discount sanity,\n",
    "- status-code distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3885ce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retailer_key</th>\n",
       "      <th>category_key</th>\n",
       "      <th>product_url</th>\n",
       "      <th>page_title</th>\n",
       "      <th>retailer_product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>gtin_ean</th>\n",
       "      <th>price_current</th>\n",
       "      <th>price_reference</th>\n",
       "      <th>discount_pct</th>\n",
       "      <th>promo_flag</th>\n",
       "      <th>stock_text_raw</th>\n",
       "      <th>delivery_promise_text</th>\n",
       "      <th>observed_at</th>\n",
       "      <th>http_status</th>\n",
       "      <th>scrape_run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/max-km01-karaoke-micro...</td>\n",
       "      <td>MAX KM01 Karaoke microfoon met speaker - Goud ...</td>\n",
       "      <td>130.139 130.139</td>\n",
       "      <td>MAX</td>\n",
       "      <td>8715693315042</td>\n",
       "      <td>13.9</td>\n",
       "      <td>17.95</td>\n",
       "      <td>22.56</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad Op voorraad Op voorraa...</td>\n",
       "      <td>Bestel en het wordt morgen bij je bezorgd! Bes...</td>\n",
       "      <td>2026-01-19T13:11:11.720276+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/vonyx-av510-bluetooth-...</td>\n",
       "      <td>Vonyx AV510 karaoke set met 2x microfoon kopen?</td>\n",
       "      <td>103.115 103.115</td>\n",
       "      <td>Vonyx</td>\n",
       "      <td>8715693319545</td>\n",
       "      <td>139.9</td>\n",
       "      <td>159.95</td>\n",
       "      <td>12.54</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad Op voorraad Op voorraa...</td>\n",
       "      <td>Bestel en het wordt morgen gratis bij je bezor...</td>\n",
       "      <td>2026-01-19T13:11:15.447600+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/vonyx-vmm100-podcast-s...</td>\n",
       "      <td>Vonyx VMM100 podcast set met mixer en accessoi...</td>\n",
       "      <td>60001097 60001097 173.403 172.638 188.020</td>\n",
       "      <td>Vonyx Vonyx Vonyx Vonyx</td>\n",
       "      <td>8720105710957 8715693293357 8715693334685 8715...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>181.35</td>\n",
       "      <td>32.73</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad Op voorraad Op voorraa...</td>\n",
       "      <td>Bestel en het wordt morgen gratis bij je bezor...</td>\n",
       "      <td>2026-01-19T13:11:16.548452+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/vonyx-wat200-draadloze...</td>\n",
       "      <td>Vonyx WAT200 draadloze zender en ontvanger voo...</td>\n",
       "      <td>179.270 179.270</td>\n",
       "      <td>Vonyx</td>\n",
       "      <td>8715693346558</td>\n",
       "      <td>59.9</td>\n",
       "      <td>69.95</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad Op voorraad</td>\n",
       "      <td>Bestel en het wordt morgen gratis bij je bezor...</td>\n",
       "      <td>2026-01-19T13:11:17.940014+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/vonyx-dm57a-dynamische...</td>\n",
       "      <td>Vonyx DM57A Dynamische zang microfoon met kabe...</td>\n",
       "      <td>173.437 173.437</td>\n",
       "      <td>Vonyx</td>\n",
       "      <td>8715693283860</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.95</td>\n",
       "      <td>12.32</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad Op voorraad Op voorraa...</td>\n",
       "      <td>Bestel en het wordt morgen bij je bezorgd! Bes...</td>\n",
       "      <td>2026-01-19T13:11:19.407992+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/vonyx-draadloze-vhf-mi...</td>\n",
       "      <td>Vonyx STWM712 set 2-kanaals met 2 microfoons k...</td>\n",
       "      <td>179.183 179.183</td>\n",
       "      <td>Vonyx</td>\n",
       "      <td>8715693255423</td>\n",
       "      <td>67.0</td>\n",
       "      <td>77.95</td>\n",
       "      <td>14.05</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad Op voorraad Op voorraa...</td>\n",
       "      <td>Bestel en het wordt morgen gratis bij je bezor...</td>\n",
       "      <td>2026-01-19T13:11:22.244530+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/vonyx-wm82-draadloze-m...</td>\n",
       "      <td>Vonyx WM82 draadloze microfoonset met twee UHF...</td>\n",
       "      <td>179.214 179.214</td>\n",
       "      <td>Vonyx</td>\n",
       "      <td>8715693301441</td>\n",
       "      <td>115.0</td>\n",
       "      <td>134.95</td>\n",
       "      <td>14.78</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad Op voorraad Op voorraa...</td>\n",
       "      <td>Bestel en het wordt morgen gratis bij je bezor...</td>\n",
       "      <td>2026-01-19T13:11:24.049311+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/vonyx-hh12-handmicrofo...</td>\n",
       "      <td>Vonyx HH12 handmicrofoon voor Vonyx UHF system...</td>\n",
       "      <td>179.253 179.253</td>\n",
       "      <td>Vonyx</td>\n",
       "      <td>8715693303551</td>\n",
       "      <td>31.0</td>\n",
       "      <td>35.95</td>\n",
       "      <td>13.77</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad</td>\n",
       "      <td>Bestel en het wordt morgen bij je bezorgd! Bes...</td>\n",
       "      <td>2026-01-19T13:11:25.659262+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/vonyx-hh10-handmicrofo...</td>\n",
       "      <td>Vonyx HH10 handmicrofoon voor Vonyx UHF system...</td>\n",
       "      <td>179.250 179.250</td>\n",
       "      <td>Vonyx</td>\n",
       "      <td>8715693303568</td>\n",
       "      <td>31.0</td>\n",
       "      <td>35.95</td>\n",
       "      <td>13.77</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad</td>\n",
       "      <td>Bestel en het wordt morgen bij je bezorgd! Bes...</td>\n",
       "      <td>2026-01-19T13:11:27.253268+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>microphones</td>\n",
       "      <td>https://www.maxiaxi.com/power-dynamics-pd504h-...</td>\n",
       "      <td>Power Dynamics PD504H microfoonsysteem met 4 m...</td>\n",
       "      <td>179.004 179.004</td>\n",
       "      <td>Power Dynamics</td>\n",
       "      <td>8715693301366</td>\n",
       "      <td>369.0</td>\n",
       "      <td>429.95</td>\n",
       "      <td>14.18</td>\n",
       "      <td>1</td>\n",
       "      <td>Op voorraad Op voorraad Op voorraad Op voorraa...</td>\n",
       "      <td>Bestel en het wordt morgen gratis bij je bezor...</td>\n",
       "      <td>2026-01-19T13:11:29.634023+00:00</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  retailer_key category_key  \\\n",
       "0      maxiaxi  microphones   \n",
       "1      maxiaxi  microphones   \n",
       "2      maxiaxi  microphones   \n",
       "3      maxiaxi  microphones   \n",
       "4      maxiaxi  microphones   \n",
       "5      maxiaxi  microphones   \n",
       "6      maxiaxi  microphones   \n",
       "7      maxiaxi  microphones   \n",
       "8      maxiaxi  microphones   \n",
       "9      maxiaxi  microphones   \n",
       "\n",
       "                                         product_url  \\\n",
       "0  https://www.maxiaxi.com/max-km01-karaoke-micro...   \n",
       "1  https://www.maxiaxi.com/vonyx-av510-bluetooth-...   \n",
       "2  https://www.maxiaxi.com/vonyx-vmm100-podcast-s...   \n",
       "3  https://www.maxiaxi.com/vonyx-wat200-draadloze...   \n",
       "4  https://www.maxiaxi.com/vonyx-dm57a-dynamische...   \n",
       "5  https://www.maxiaxi.com/vonyx-draadloze-vhf-mi...   \n",
       "6  https://www.maxiaxi.com/vonyx-wm82-draadloze-m...   \n",
       "7  https://www.maxiaxi.com/vonyx-hh12-handmicrofo...   \n",
       "8  https://www.maxiaxi.com/vonyx-hh10-handmicrofo...   \n",
       "9  https://www.maxiaxi.com/power-dynamics-pd504h-...   \n",
       "\n",
       "                                          page_title  \\\n",
       "0  MAX KM01 Karaoke microfoon met speaker - Goud ...   \n",
       "1    Vonyx AV510 karaoke set met 2x microfoon kopen?   \n",
       "2  Vonyx VMM100 podcast set met mixer en accessoi...   \n",
       "3  Vonyx WAT200 draadloze zender en ontvanger voo...   \n",
       "4  Vonyx DM57A Dynamische zang microfoon met kabe...   \n",
       "5  Vonyx STWM712 set 2-kanaals met 2 microfoons k...   \n",
       "6  Vonyx WM82 draadloze microfoonset met twee UHF...   \n",
       "7  Vonyx HH12 handmicrofoon voor Vonyx UHF system...   \n",
       "8  Vonyx HH10 handmicrofoon voor Vonyx UHF system...   \n",
       "9  Power Dynamics PD504H microfoonsysteem met 4 m...   \n",
       "\n",
       "                         retailer_product_id                    brand  \\\n",
       "0                            130.139 130.139                      MAX   \n",
       "1                            103.115 103.115                    Vonyx   \n",
       "2  60001097 60001097 173.403 172.638 188.020  Vonyx Vonyx Vonyx Vonyx   \n",
       "3                            179.270 179.270                    Vonyx   \n",
       "4                            173.437 173.437                    Vonyx   \n",
       "5                            179.183 179.183                    Vonyx   \n",
       "6                            179.214 179.214                    Vonyx   \n",
       "7                            179.253 179.253                    Vonyx   \n",
       "8                            179.250 179.250                    Vonyx   \n",
       "9                            179.004 179.004           Power Dynamics   \n",
       "\n",
       "                                            gtin_ean  price_current  \\\n",
       "0                                      8715693315042           13.9   \n",
       "1                                      8715693319545          139.9   \n",
       "2  8720105710957 8715693293357 8715693334685 8715...          122.0   \n",
       "3                                      8715693346558           59.9   \n",
       "4                                      8715693283860           21.0   \n",
       "5                                      8715693255423           67.0   \n",
       "6                                      8715693301441          115.0   \n",
       "7                                      8715693303551           31.0   \n",
       "8                                      8715693303568           31.0   \n",
       "9                                      8715693301366          369.0   \n",
       "\n",
       "   price_reference  discount_pct  promo_flag  \\\n",
       "0            17.95         22.56           1   \n",
       "1           159.95         12.54           1   \n",
       "2           181.35         32.73           1   \n",
       "3            69.95         14.37           1   \n",
       "4            23.95         12.32           1   \n",
       "5            77.95         14.05           1   \n",
       "6           134.95         14.78           1   \n",
       "7            35.95         13.77           1   \n",
       "8            35.95         13.77           1   \n",
       "9           429.95         14.18           1   \n",
       "\n",
       "                                      stock_text_raw  \\\n",
       "0  Op voorraad Op voorraad Op voorraad Op voorraa...   \n",
       "1  Op voorraad Op voorraad Op voorraad Op voorraa...   \n",
       "2  Op voorraad Op voorraad Op voorraad Op voorraa...   \n",
       "3                Op voorraad Op voorraad Op voorraad   \n",
       "4  Op voorraad Op voorraad Op voorraad Op voorraa...   \n",
       "5  Op voorraad Op voorraad Op voorraad Op voorraa...   \n",
       "6  Op voorraad Op voorraad Op voorraad Op voorraa...   \n",
       "7                            Op voorraad Op voorraad   \n",
       "8                            Op voorraad Op voorraad   \n",
       "9  Op voorraad Op voorraad Op voorraad Op voorraa...   \n",
       "\n",
       "                               delivery_promise_text  \\\n",
       "0  Bestel en het wordt morgen bij je bezorgd! Bes...   \n",
       "1  Bestel en het wordt morgen gratis bij je bezor...   \n",
       "2  Bestel en het wordt morgen gratis bij je bezor...   \n",
       "3  Bestel en het wordt morgen gratis bij je bezor...   \n",
       "4  Bestel en het wordt morgen bij je bezorgd! Bes...   \n",
       "5  Bestel en het wordt morgen gratis bij je bezor...   \n",
       "6  Bestel en het wordt morgen gratis bij je bezor...   \n",
       "7  Bestel en het wordt morgen bij je bezorgd! Bes...   \n",
       "8  Bestel en het wordt morgen bij je bezorgd! Bes...   \n",
       "9  Bestel en het wordt morgen gratis bij je bezor...   \n",
       "\n",
       "                        observed_at  http_status    scrape_run_id  \n",
       "0  2026-01-19T13:11:11.720276+00:00          200  20260119_141105  \n",
       "1  2026-01-19T13:11:15.447600+00:00          200  20260119_141105  \n",
       "2  2026-01-19T13:11:16.548452+00:00          200  20260119_141105  \n",
       "3  2026-01-19T13:11:17.940014+00:00          200  20260119_141105  \n",
       "4  2026-01-19T13:11:19.407992+00:00          200  20260119_141105  \n",
       "5  2026-01-19T13:11:22.244530+00:00          200  20260119_141105  \n",
       "6  2026-01-19T13:11:24.049311+00:00          200  20260119_141105  \n",
       "7  2026-01-19T13:11:25.659262+00:00          200  20260119_141105  \n",
       "8  2026-01-19T13:11:27.253268+00:00          200  20260119_141105  \n",
       "9  2026-01-19T13:11:29.634023+00:00          200  20260119_141105  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retailer_key</th>\n",
       "      <th>page_type</th>\n",
       "      <th>source_url</th>\n",
       "      <th>captured_at</th>\n",
       "      <th>content_text</th>\n",
       "      <th>http_status</th>\n",
       "      <th>scrape_run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>maxiaxi</td>\n",
       "      <td>shipping_returns</td>\n",
       "      <td>https://www.maxiaxi.com/klantenservice/</td>\n",
       "      <td>2026-01-19T13:11:07.476845+00:00</td>\n",
       "      <td>The store will not work correctly when cookies...</td>\n",
       "      <td>200</td>\n",
       "      <td>20260119_141105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  retailer_key         page_type                               source_url  \\\n",
       "0      maxiaxi  shipping_returns  https://www.maxiaxi.com/klantenservice/   \n",
       "\n",
       "                        captured_at  \\\n",
       "0  2026-01-19T13:11:07.476845+00:00   \n",
       "\n",
       "                                        content_text  http_status  \\\n",
       "0  The store will not work correctly when cookies...          200   \n",
       "\n",
       "     scrape_run_id  \n",
       "0  20260119_141105  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 187\n",
      "\n",
      "Rows per retailer:\n",
      "retailer_key\n",
      "maxiaxi    187\n",
      "Name: count, dtype: int64\n",
      "\n",
      "HTTP status distribution:\n",
      "http_status\n",
      "200    187\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing price_current %: 0.0\n",
      "Discount pct summary:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    187.000000\n",
       "mean      15.708770\n",
       "std        6.688904\n",
       "min        0.000000\n",
       "25%       12.035000\n",
       "50%       14.420000\n",
       "75%       18.875000\n",
       "max       37.750000\n",
       "Name: discount_pct, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8. Load and QA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_prod = pd.read_csv(PROD_CSV_PATH) if PROD_CSV_PATH.exists() else pd.DataFrame()\n",
    "df_ret  = pd.read_csv(RET_CSV_PATH)  if RET_CSV_PATH.exists()  else pd.DataFrame()\n",
    "\n",
    "display(df_prod.head(10))\n",
    "display(df_ret.head(10))\n",
    "\n",
    "if not df_prod.empty:\n",
    "    print(\"Rows:\", len(df_prod))\n",
    "    print(\"\\nRows per retailer:\")\n",
    "    print(df_prod[\"retailer_key\"].value_counts(dropna=False))\n",
    "\n",
    "    print(\"\\nHTTP status distribution:\")\n",
    "    print(df_prod[\"http_status\"].value_counts(dropna=False).head(10))\n",
    "\n",
    "    print(\"\\nMissing price_current %:\", round(df_prod[\"price_current\"].isna().mean()*100, 2))\n",
    "    print(\"Discount pct summary:\")\n",
    "    display(df_prod[\"discount_pct\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba96d4",
   "metadata": {},
   "source": [
    "# 9. (Optional) Export Relational Tables to CSV for the Report Appendix\n",
    "\n",
    "This is useful for a “deliverables bundle”:\n",
    "- raw observations (time series),\n",
    "- product_page registry,\n",
    "- retailer page captures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f781ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: /Users/feddekoster/Desktop/ODM_Assignment2/retailer_20260119_141105.csv rows: 1\n",
      "Exported: /Users/feddekoster/Desktop/ODM_Assignment2/category_20260119_141105.csv rows: 1\n",
      "Exported: /Users/feddekoster/Desktop/ODM_Assignment2/product_page_20260119_141105.csv rows: 187\n",
      "Exported: /Users/feddekoster/Desktop/ODM_Assignment2/offer_observation_20260119_141105.csv rows: 187\n",
      "Exported: /Users/feddekoster/Desktop/ODM_Assignment2/retailer_page_capture_20260119_141105.csv rows: 1\n"
     ]
    }
   ],
   "source": [
    "# 9. Export DB tables\n",
    "\n",
    "with db_connect() as con:\n",
    "    for table in [\"retailer\", \"category\", \"product_page\", \"offer_observation\", \"retailer_page_capture\"]:\n",
    "        df = pd.read_sql_query(f\"SELECT * FROM {table}\", con)\n",
    "        out = OUTPUT_DIR / f\"{table}_{RUN_ID}.csv\"\n",
    "        df.to_csv(out, index=False)\n",
    "        print(\"Exported:\", out, \"rows:\", len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
